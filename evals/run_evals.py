"""
Main script to run evaluations with Phoenix.
"""
import json
import sys
import os
import pandas as pd
from typing import List, Dict, Any

# Add project root to path so we can import modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from evals.tool_calling.test_cases import TEST_CASES
from tools import PositioningTool, ScrapingTool, SlackTool, RAGTool

# Phoenix and related imports
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from openinference.instrumentation.langchain import LangChainInstrumentor

# Mock classes for dependencies
class MockVectorStore:
    def similarity_search(self, *args, **kwargs):
        return []

class MockScrapingService:
    def analyze_website(self, *args, **kwargs):
        return {"name": "Mock Product", "description": "Mock Description", 
                "pain_points": ["Mock Pain Point"], "pricing": "Free", 
                "target_audience": "Developers"}

class MockDocumentService:
    def process_competitor(self, *args, **kwargs):
        return True

def format_tool_definitions(tools: List) -> str:
    """Format tool definitions for the evaluation prompt."""
    tool_defs = []
    for tool in tools:
        tool_def = {
            "name": tool.name,
            "description": tool.description,
        }
        tool_defs.append(tool_def)
    return json.dumps(tool_defs, indent=2)

def simulate_tool_call(case: Dict[str, Any]) -> Dict[str, Any]:
    """Simulate a tool call based on test case expected values."""
    return {
        "name": case["expected_tool"],
        "parameters": case["expected_params"]
    }

def run_tool_calling_evals():
    """Run tool calling evaluations without Phoenix."""
    print("Running tool calling evaluations...")
    
    # Initialize tools with mock dependencies
    mock_vector_store = MockVectorStore()
    mock_scraping_service = MockScrapingService()
    mock_document_service = MockDocumentService()
    
    tools = [
        PositioningTool(vector_store=mock_vector_store),
        ScrapingTool(scraping_service=mock_scraping_service, document_service=mock_document_service),
        SlackTool(),
        RAGTool(vector_store=mock_vector_store)
    ]
    
    # Create dataframe from test cases
    eval_data = []
    for case in TEST_CASES:
        tool_call = simulate_tool_call(case)
        eval_data.append({
            "question": case["question"],
            "tool_call": json.dumps(tool_call, indent=2),
        })
    
    df = pd.DataFrame(eval_data)
    
    # Initialize evaluator
    evaluator = ChatOpenAI(model="gpt-4", temperature=0)
    
    # Custom tool calling prompt template
    TOOL_CALLING_PROMPT_TEMPLATE = """
    You are an evaluation assistant evaluating questions and tool calls to
    determine whether the tool called would answer the question. The tool
    calls have been generated by a separate agent, and chosen from the list of
    tools provided below. It is your job to decide whether that agent chose
    the right tool to call.

        [BEGIN DATA]
        ************
        [Question]: {question}
        ************
        [Tool Called]: {tool_call}
        [END DATA]

    Your response must be single word, either "correct" or "incorrect",
    and should not contain any text or characters aside from that word.
    "incorrect" means that the chosen tool would not answer the question,
    the tool includes information that is not presented in the question,
    or that the tool signature includes parameter values that don't match
    the formats specified in the tool signatures below.

    "correct" means the correct tool call was chosen, the correct parameters
    were extracted from the question, the tool call generated is runnable and correct,
    and that no outside information not present in the question was used
    in the generated question.

        [Tool Definitions]: {tool_definitions}
    """
    
    # Format tool definitions
    json_tools = format_tool_definitions(tools)
    
    # Run evaluations
    results = []
    for idx, row in df.iterrows():
        print(f"Evaluating test case {idx+1}/{len(df)}...")
        
        prompt = TOOL_CALLING_PROMPT_TEMPLATE.format(
            question=row["question"],
            tool_call=row["tool_call"],
            tool_definitions=json_tools
        )
        
        response = evaluator.invoke(prompt)
        result = response.content.strip().lower()
        
        # Validate response
        if result not in ["correct", "incorrect"]:
            print(f"Warning: Invalid evaluation result: {result}, defaulting to 'incorrect'")
            result = "incorrect"
            
        results.append({
            "question": row["question"],
            "tool_call": row["tool_call"],
            "evaluation": result,
            "is_correct": result == "correct"
        })
    
    results_df = pd.DataFrame(results)
    
    # Calculate and print results
    correct_count = results_df["is_correct"].sum()
    total = len(results_df)
    accuracy = correct_count / total if total > 0 else 0
    
    print(f"Tool calling accuracy: {accuracy:.2%} ({correct_count}/{total})")
    print("Detailed results:")
    print(results_df[["question", "evaluation"]].to_string())
    
    # Save results to CSV
    results_df.to_csv("evals/tool_calling_eval_results.csv", index=False)
    
    return results_df

if __name__ == "__main__":
    run_tool_calling_evals() 